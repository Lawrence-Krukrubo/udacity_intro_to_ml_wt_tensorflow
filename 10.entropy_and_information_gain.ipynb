{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy:\n",
    "Entropy comes from Physics. It precisely measures howmuch freedom does a particle have to move around?<br>For example considering the 3 states of water:-\n",
    "* Solid (Ice)\n",
    "* Liquid (A drink)\n",
    "* Gas (Vapour)\n",
    "\n",
    "Thinking of the particles in Ice, Water and Vapour, we can surmise that:- \n",
    "* Ice is pretty rigid, in that its' particles don't have many places to go. They mostly stay where they are.\n",
    "* Liquid is a lil less rigid as the particles have a few places to move around,\n",
    "* Water vapour particles obviously have a lot more freedom to move around the atmosphere.\n",
    "\n",
    "So Entropy measures precisely this, how much freedom does a particle have to move around. Thus the Entropy of ice is low, the Entropy of liquid water is medium and the Entropy of gaseous vapour is high.\n",
    "\n",
    "In the field of probability it can be said that the more rigid or homogenous a set is, the less entropy it will have and vice-versa.\n",
    "\n",
    "Another way to think of Entropy is in terms of knowledge, for example, if we were to pick a random ball from a bucket, how much do we know about the color composition of the balls in each bucket?\n",
    "* **bucket** 1 = 4 red balls\n",
    "* **bucket** 2 = 3 red, 1 blue balls\n",
    "* **bucket** 3 = 2 red, 2 blue balls\n",
    "\n",
    "In the 1st bucket, we know for sure that the ball will  be red, thus we have _high-knowledge_. In the 2nd bucket we know that the ball is more likely to be red, so we have _medium knowledge_, while in the 3rd bucket, the ball is as likely to be red as blue, so we have _little knowledge_ about the possible outcome.\n",
    "\n",
    "Knowledge in this case is opposite to Entropy, in that, since we're more certain of the outcome of the 1st bucket, we have low-Entropy and medium-Entropy for bucket 2 and high-Entropy for bucket 1, whose outcome we're least certain of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Entropy:\n",
    "If we picked and dropped back each ball into the respective buckets, \n",
    "* What's the probability of picking 4 red balls from the 1st bucket:=> $1*1*1*1 = 1$, (100%)\n",
    "* What's the probability of picking 3 red and 1 blue balls from the 2nd bucket:=> $0.75*0.75*0.75*0.25 = 0.105$, (10%)\n",
    "* What's the probability of picking 2 red and 2 blue balls from the 3rd bucket:=> $0.5*0.5*0.5*0.5 = 0.0625$, (6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of working with probabilities, which are rational numbers between 0 and 1 is that if we have many elements in each set or bucket, multiplying them over will lead to a very small irrational number difficult to work with. Therefore we need a way to avoid irrational numbers from repeated long, convoluted multiplications.\n",
    "\n",
    "The solution is to take the $log_2{(x)}$ where $x$ is the computed probability.\n",
    "\n",
    "Thus from our 3 buckets above:<br>\n",
    "* $log_2{(1)} = 0$\n",
    "* $log_2{(0.105)} = -3.2515$\n",
    "* $log_2{(0.0625)} = -4$\n",
    "\n",
    "Since most of the numbers will be negative, as the $log_2{(x)}$ is negative when $x$ < 1, we'd just take the negative log of these numbers. Therefore our values will now be\n",
    "* 0\n",
    "* 3.2515\n",
    "* 4\n",
    "\n",
    "Finally, we just take the average score for each value, by dividing each by 4, since there're 4 balls in each bucket\n",
    "* $\\frac{0}{4}=0$\n",
    "\n",
    "* $\\frac{3.2515}{4}=0.81$\n",
    "\n",
    "* $\\frac{4}{4}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by so doing, we have computed the Entropy for each bucket above.<br>To compute the Entropy, we take the **average of the negative of the log base 2 of the probabilities** of picking the balls in the right fashion from each bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Formula ideal for Multi-Class Entropy:\n",
    "\n",
    "## $-\\sum_{i=1}^np_ilog_2(p_i)$\n",
    "\n",
    "Where $p_i$ is the probability of the $i$th item from $i=1$ through $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(*args):\n",
    "    \"\"\"This Method calculates the entropy\n",
    "        of a set of values and returns it\n",
    "    \"\"\"\n",
    "    total = sum(args)\n",
    "    entropy = 0\n",
    "    for val in args:\n",
    "        temp = -(val/total)*np.log2(val/total)\n",
    "        entropy+=temp\n",
    "    \n",
    "    return np.round(entropy, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EX**<br>\n",
    "There are 5 red balls and 3 blue balls in a bucket. If 8 balls were taken one after another with replacement, calculate the entropy of picking exactly 5 red balls and 3 blue balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 5\n",
    "b = 3\n",
    "\n",
    "calculate_entropy(r,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the entropy for a bucket that contains 4 red balls and 10 blue balls\n",
    "r = 4\n",
    "b = 10\n",
    "calculate_entropy(r, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3347"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we have a bucket with eight red balls, three blue balls, and two yellow balls, \n",
    "# what is the entropy of the set of balls? Input your answer to at least three decimal places.\n",
    "\n",
    "r = 8\n",
    "b = 3\n",
    "y = 2\n",
    "\n",
    "calculate_entropy(r, b, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain:\n",
    "\n",
    "The formula for information gain is very simple... It's just the change in entropy. To be specific...<br>For every node in the decision tree, we can calculate the entropy of the data in the parent node and then we calculate the entropies of the two children nodes. The information gain is the difference between the entropy of the parent node and the average of the two children nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
