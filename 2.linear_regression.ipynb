{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Absolute Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This assumes that the point in question wants the line to come closer**\n",
    "\n",
    "Recall that a line has a _slope_ and a _y-intercept_, in the form $y = mx + b$, where $m$ is the slope and $b$ the _y-intercept_\n",
    "\n",
    "We also assume that the `learning-rate` or `alpha` must be positive.\n",
    "\n",
    "1. The first step is to figure out if the point is lower or higher than the line from the linear equation\n",
    "\n",
    "2. If the point is lower, we reduce the _y-intercept_ by the _learning_rate_ to lower the line towards the point and we subtract the `x-value times the alpha` from the slope to tilt it towards the point.\n",
    "\n",
    "3. If the point is higher, we increase the _y-intercept_ by the _learning_rate_ to lift the line towards the point and we add the `x-value times the alpha` to the slope to tilt it towards the point.\n",
    "\n",
    "4. Thus to tilt the slope, _Absolute-Trick_ exclusively adds or subtracts the `x-value times the alpha` from the slope.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_trick(line_equ, point_coord, alpha):\n",
    "    \"\"\"Find the new line equation using the\n",
    "        Absolute-Trick formula\n",
    "        \n",
    "    @param line_equ: tuple of floats for slope and y_intercept\n",
    "    @param point_coord: tuple of floats for 2D point coordinates\n",
    "    @param alpha: float, the learning-rate\n",
    "    \"\"\"\n",
    "    assert alpha >= 0, 'ERROR: Alpha must be positive'\n",
    "    \n",
    "    slope, intercept = line_equ[0], line_equ[1]\n",
    "    x, y = point_coord[0], point_coord[1]\n",
    "    \n",
    "    # if point is higher than line, is_higher=True, else False.\n",
    "    is_higher = y > (slope*x + intercept) \n",
    "    print(f'Old-Equ: y = {slope}x + {intercept},\\nIs_Point_Higher: {is_higher}\\n')\n",
    "    \n",
    "    if is_higher:\n",
    "        intercept += alpha\n",
    "        slope += x*alpha\n",
    "    \n",
    "    else:\n",
    "        intercept -= alpha\n",
    "        slope -= x*alpha\n",
    "    \n",
    "    print(f'New-Equ: y = {round(slope, 2)}x + {round(intercept,2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Equ: y = -0.6x + 4,\n",
      "Is_Point_Higher: False\n",
      "\n",
      "New-Equ: y = -0.1x + 3.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "line_equ = (-0.6, 4)\n",
    "point_coord = (-5, 3)\n",
    "\n",
    "absolute_trick(line_equ, point_coord, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Square Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This assumes that the point in question also wants the line to come closer**\n",
    "\n",
    "Recall that a line has a _slope_ and a _y-intercept_, in the form $y = mx + b$, where $m$ is the slope and $b$ the _y-intercept_\n",
    "\n",
    "We also assume that the `learning-rate` or `alpha` must be positive.\n",
    "\n",
    "1. The main difference between the _Absolute-Trick_ and the _Square-Trick_ is that in addition to the distance on the _x-axis_, the _Square-Trick_ compares the distance on the _y-axis_ between the point and the line\n",
    "\n",
    "1. The first step is to consider the difference between the $y$ value of the point and the $y$ value of the line when the $x$'s are the same.\n",
    "\n",
    "2. This difference is multiplied to the `x-value of the point times alpha` and added to the slope to get the new line-equation slope \n",
    "\n",
    "3. This difference is multiplied to the _alpha_ and added to the _y_intercept_ to get the new line-equation _y_intercept_\n",
    "\n",
    "4. This automatically moves the line closer to the point irrespective of whether the point is higher or lower than the line\n",
    "\n",
    "5. Including this distance in the calculation, helps the line to take the right size of steps towards the point than the relatively fixed sizes based on the _alpha_ in the _Absolute-Trick_. In other words, if the point is far from the line, the _Square-Trick_ makes the line move more than if the point was closer to the line.\n",
    "6.  Finally, the _Square-Trick_ automatically takes care of points that may be higher or lower than the line and so, we don't need to have two rules like the `if-else` statements of _Absolute-Trick_.\n",
    "\n",
    "**Thus in Square-Trick, The magnitude by which the intercept and slope change is dependent on how large the error in prediction is.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_trick(line_equ, point_coord, alpha):\n",
    "    \"\"\"Find the new line equation using the\n",
    "        Absolute-Trick formula\n",
    "        \n",
    "    @param line_equ: tuple of floats for slope and y_intercept\n",
    "    @param point_coord: tuple of floats for point coordinates\n",
    "    @param alpha: float, the learning-rate\n",
    "    \"\"\"\n",
    "    assert alpha >= 0, 'ERROR: Alpha must be positive'\n",
    "    \n",
    "    slope, intercept = line_equ[0], line_equ[1]\n",
    "    x, y = point_coord[0], point_coord[1]\n",
    "    \n",
    "    # let's just see if the point is higher or not\n",
    "    is_higher = y > (slope*x + intercept) \n",
    "    print(f'Old-Equ: y = {slope}x + {intercept},\\nIs_Point_Higher: {is_higher}\\n')\n",
    "    \n",
    "    # calculate the distance from the point to the line,\n",
    "    # On the y-axis\n",
    "    dist = y - (slope*x + intercept)\n",
    "    \n",
    "    # Include dist in the new-line Equation\n",
    "    slope += x*dist*alpha\n",
    "    intercept += dist*alpha\n",
    "    \n",
    "    print(f'New-Equ: y = {round(slope, 2)}x + {round(intercept, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Equ: y = 2x + 3,\n",
      "Is_Point_Higher: True\n",
      "\n",
      "New-Equ: y = 2.1x + 3.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "line_equ = (2, 3)\n",
    "point_coord = (5, 15)\n",
    "\n",
    "square_trick(line_equ, point_coord, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Equ: y = -0.6x + 4,\n",
      "Is_Point_Higher: False\n",
      "\n",
      "New-Equ: y = -0.4x + 3.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "line_equ = (-0.6, 4)\n",
    "point_coord = (-5, 3)\n",
    "\n",
    "square_trick(line_equ, point_coord, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent:\n",
    "\n",
    "1. First for each point, we calculate the gradient of the chosen error function in respect to the weights. This error function could be MAE or MSE for example. This gradient is the biggest distance from that point to the line or global minimum. \n",
    "2. Next, we take the negative of the gradient and move in this negative or opposite direction as this is the direction that minimizes the gradient distance the most\n",
    "3. Usually this is such a big step and in ML generally we avoid taking too big steps so as not to over shoot the minimum, so we multiply this negative-gradient value by the alpha or learning rate and move in the right direction gradually and repetitively.\n",
    "4. **Note That:** The Gradient Descent takes into consideration the following:\n",
    "* 1. The derivative of the error function in respect to the _weight_ $w1$\n",
    "* 2. The derivative of the error function in respect to the _y-intercept_ $w2$\n",
    "* In short, it takes the derivatives of the error or loss function in respect to the existing weights and bias.\n",
    "5. These calculations are exactly the same as the _Absolute-Trick_ or _Squared-Trick_ we computed earlier\n",
    "6. Note that a gradient is a vector, so it has both of the following characteristics:\n",
    "* A direction\n",
    "* A magnitude\n",
    "\n",
    "The gradient always points in the direction of steepest increase in the loss function. Thus, gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Functions\n",
    "\n",
    "#### Mean Absolute Error (MAE) and Mean Squared Error (MSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. MAE:**\n",
    "\n",
    "This is the sum or total absolute errors between each point $y$ and the prediction line's point $\\hat{y}$, divided by the total number of points $m$.\n",
    "\n",
    "## MAE = $\\frac{1}{m}\\sum_{i=1}^m{|y - \\hat{y}|}$\n",
    "\n",
    "* We take the absolute values of the errors or differences between $y$ and $\\hat{y}$ so that we don't have any negative values that could in addition, cancel out positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. MSE:**\n",
    "\n",
    "This is the sum or total squared errors between each point $y$ and the prediction line's point $\\hat{y}$, divided by the total number of points $m$, multiplied by $\\frac{1}{2}$. The half is just for convenience for calculating _Gradient Descent_.\n",
    "\n",
    "## MSE = $\\frac{1}{2m}\\sum_{i=1}^m{(y - \\hat{y})}^2$\n",
    "\n",
    "* We take the squared values of the errors or differences between $y$ and $\\hat{y}$ so that we don't have any negative values,as squared values must be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz for Mean Absolute Error**\n",
    "\n",
    "Compute the mean absolute error for the following line and points:\n",
    "\n",
    "line: `y = 1.2x + 2`\n",
    "\n",
    "points: `(2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(line_equ, point_coords):\n",
    "    \"\"\"Find the MAE between points and a line\n",
    "        \n",
    "    @param line_equ: a tuple of numbers for slope and y_intercept\n",
    "    @param point_coords: a list of tuples of numbers for point coordinates\n",
    "    @return: MAE, a number\n",
    "    \"\"\"\n",
    "    slope, intercept = line_equ[0], line_equ[1]\n",
    "    xes = [tup[0] for tup in point_coords]\n",
    "    yes = [tup[1] for tup in point_coords]\n",
    "    mae_sum = 0\n",
    "    print(f'Line-Equ: y = {slope}x + {intercept}\\n')\n",
    "    \n",
    "    # calculate the distance from each point to the line,\n",
    "    # On the y-axis\n",
    "    for x, y in zip(xes, yes):  \n",
    "        y_hat = slope*x + intercept\n",
    "        mae_dist = np.abs(y - y_hat)\n",
    "        mae_sum+=mae_dist\n",
    "    \n",
    "    return mae_sum / len(point_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line-Equ: y = 1.2x + 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.88"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_equ = (1.2, 2)\n",
    "point_coords = [(2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)]\n",
    "\n",
    "calc_mae(line_equ, point_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mse(line_equ, point_coords):\n",
    "    \"\"\"Find the MSE between points and a line\n",
    "        \n",
    "    @param line_equ: a tuple of numbers for slope and y_intercept\n",
    "    @param point_coords: a list of tuples of numbers for point coordinates\n",
    "    @return; a list of MAE number scores\n",
    "    \"\"\"\n",
    "    slope, intercept = line_equ[0], line_equ[1]\n",
    "    xes = [tup[0] for tup in point_coords]\n",
    "    yes = [tup[1] for tup in point_coords]\n",
    "    mse_sum = 0\n",
    "    print(f'Line-Equ: y = {slope}x + {intercept}\\n')\n",
    "    \n",
    "    # calculate the distance from each point to the line,\n",
    "    # On the y-axis\n",
    "    for x, y in zip(xes, yes):  \n",
    "        y_hat = slope*x + intercept\n",
    "        mse_dist = np.power(y - y_hat, 2)\n",
    "        mse_sum+=mse_dist\n",
    "    \n",
    "    return round((mse_sum / len(point_coords))*0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line-Equ: y = 1.2x + 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.69"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mse(line_equ, point_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof: \n",
    "That Minimizing The Error with Gradient Descent is exactly same as minimizing with Absolute or Square Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{dx}{dw_1}Error => -(y - \\hat{y})x$\n",
    "\n",
    "## $\\frac{dx}{dw_2}Error => -(y - \\hat{y})$\n",
    "\n",
    "The above two equations mean that in calculating _Gradient Descent_, the derivative of the error function with respect to the weight or slope $w_1$ is equal to the negative of $y - \\hat{y}$ multiplied by $x$ as the updated slope, while the derivative of the error function in respect to the y-intercept or bias unit $w_2$ is equal to the negative of $y - \\hat{y}$ as the new y-intercept.\n",
    "\n",
    "Let's try to do this with some concrete examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Equ: y = 2x + 3,\n",
      "Is_Point_Higher: True\n",
      "\n",
      "New-Equ: y = 2.1x + 3.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "line_equ = (2, 3)\n",
    "point_coord = (5, 15)\n",
    "\n",
    "square_trick(line_equ, point_coord, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat: 13, mse: 2.0\n",
      "slope: 2, bias: 3\n"
     ]
    }
   ],
   "source": [
    "x, y = point_coord[0], point_coord[1]\n",
    "slope, bias = line_equ[0], line_equ[1]\n",
    "\n",
    "y_hat = slope*x + bias\n",
    "mse = ((y - y_hat)**2)*0.5\n",
    "print(f'yhat: {y_hat}, mse: {mse}')\n",
    "print(f'slope: {slope}, bias: {bias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = 2.1*x + 3.02\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-slope: -10, new_bias: -2\n"
     ]
    }
   ],
   "source": [
    "new_slope = -(y - y_hat)*x\n",
    "new_bias = -(y - y_hat)\n",
    "\n",
    "print(f'new-slope: {new_slope}, new_bias: {new_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy dot and matmul functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy.dot() function is used for performing matrix multiplication in Python. It also checks the condition for matrix multiplication, that is, the number of columns of the first matrix must be equal to the number of the rows of the second. It works with multi-dimensional arrays also. We can also specify an alternate array as a parameter to store the result. The @ operator for multiplication invokes the matmul() function of an array that is used to perform the same multiplication. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 3]]\n",
      " \n",
      "[[8 4]\n",
      " [4 7]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2], [2, 3]])\n",
    "b = np.array([[8,4], [4, 7]])\n",
    "\n",
    "print(a)\n",
    "print(' ')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 18]\n",
      " [28 29]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 18],\n",
       "       [28, 29]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a@b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
